# rate-limiter
## Описание алгоритма:
Реализуем rate-limiter, который пропускает не больше заданного числа запросов в заданый
промежуток времени. Id временного окна рассчитываем как текущее время в миллисекнудах,
деленное на размер окна. Id уникален для любого временного окна.
Какое-то время храним id прошедших временных окон и значения счетчиков запросов для них,
предполагается, что неактульные временные окна и их счетчики удаляются по расписанию
демон-процессом.
## Доказательство корректности:
Предполагается использование rate-limiter'а нескольскими потоками одновременно,
для того, чтобы это было возможно, используем ConcurrentHashMap, а в качестве
счетчика количества запросов во временном окне используем AtomicInteger, чтобы 
инкрементирование счетчика было атомарной операцией, и мы не теряли значения и 
счетчик всегда показывал актуальную информацию о количестве одобренных запросов.
Цель алгоритма - чтобы не больше, чем заданное число запросов в данном окне,
получали разрешение на выполнение. Так как управление у потока может быть 
отобрано в любой момент и на неопределенное время, перед тем, как возвращать
результат, мы проверяем что мы находимся все еще в данном временном окне, в противном
случае наше условие на количество запросов во временное окно может быть нарушено.
Также в нашем rate-limiter'е предусмотрена возможность динамического изменения 
допустимого количества запросов за временное окно, чтобы все потоки одновременно
увидели изменение этой переменной, она объевлена как volatile
## Идея распределенного rate-limiter'а
У нас есть кластер с большим количеством сервисов, которые отправляют сообщения пользователям,
мы хотим, чтобы наши сервисы не отправляли сообщения пользователям слишком часто, чтобы не
перегружать их. Для этого в нашей системе появится специальный компонент rate-limiter,
перед каждой отправкой сообщения пользователю, мы будем спрашивать у него "разрешения".
![](https://github.com/cvbnnthc1/rate-limiter/blob/master/pictures/schema1.jpeg)
Таким образом, rate-limiter может превратиться в единую точку отказа системы, поэтому стоит 
его сделать высокодоступным и надежным. Для этого сделаем несколько реплик rate-limiter'а,
добавим L7-балансировщик. который также будет выполнять функции service-registry -
следить за состоянием реплик.
![](https://github.com/cvbnnthc1/rate-limiter/blob/master/pictures/schema2.jpeg)
Для каждой реплики у нас будут отдельные счетчики запросов, так же считаем количество 
запросов для каждого пользователя, чтобы не отправить слишком много сообщений.
Когда rate-limiter становится распределенным, возникает важная проблема: если одна или
несколько реплик упадут, нужно увеличивать допустимые лимиты запросов на остальных репликах,
иначе мы рискуем слать меньше трафика, чем должны. Для этого service registry будет рассылать
репликам коэффициент, во сколько раз нужно поменять лимит на количество запросов во временное 
окно. В NoSQL-бд будут хранится настройки rate-limiter'а для каждого пользователя, также для
скорости эти настройки будут дублироваться в in-memory cache, чтобы реплики могли быстро пересчитать
лимиты при падении узлов или при возвращении узлов в балансировку.
## Плюсы подхода
- Простая архитектура
- rate-limiter работоспобосен даже в том случае, когда часть узлов вышла из строя
- адаптируем лимиты в зависимости от количества реплик
- передаем мало данных по сети (id пользователя или команду на изменение размера лимитов)
## Минусы подхода
- сложно точно определить, сколько запросов успело пройти через упавшие реплики, можем
отправить больше запросов, чем должны были
- необходимо регулярно чистить счетчики для неактуальных окон
- не совсем гибкая настройка, запросы по сервисам могут распределяться не честно,
пришлось идти на компромисс, так как, если бы заводили счетчик для каждого сервиса и 
пользователя, при падении части реплик пришлось бы более долго обновлять лимиты
